{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ScreenShot](images/loading_screen.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3 - Behavioral Cloning\n",
    "### _How to Train a Deep Neural Network to Drive a Simulated Car_ \n",
    "#### Please refer to this repository's README.md for a detailed explanation of the project: https://github.com/nhiddink/CarND_P3_Behavioral_Cloning/blob/master/README.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# http://pandas.pydata.org/\n",
    "import pandas as pd\n",
    "\n",
    "# http://www.numpy.org/\n",
    "import numpy as np\n",
    "\n",
    "# http://docs.opencv.org/3.0-beta/doc/py_tutorials/py_gui/py_image_display/py_image_display.html\n",
    "import cv2\n",
    "\n",
    "# http://matplotlib.org/\n",
    "import matplotlib\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# http://scikit-learn.org/stable/\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# https://keras.io/\n",
    "from keras.models import load_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout,Convolution2D,MaxPooling2D,Flatten,Lambda\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Entry from driving_log.csv: \n",
      "                                   center  \\\n",
      "0  IMG/center_2016_12_01_13_30_48_287.jpg   \n",
      "\n",
      "                                    left  \\\n",
      "0   IMG/left_2016_12_01_13_30_48_287.jpg   \n",
      "\n",
      "                                    right  steering  throttle  brake     speed  \n",
      "0   IMG/right_2016_12_01_13_30_48_287.jpg       0.0       0.0    0.0  22.14829  \n"
     ]
    }
   ],
   "source": [
    "# Declare variables\n",
    "folder = './data/'\n",
    "csv_path = 'driving_log.csv'\n",
    "\n",
    "# Load data from driving_log.csv using Pandas\n",
    "training_data = pd.read_csv(folder + csv_path, delimiter=None, header='infer', names=None)\n",
    "print('Sample Entry from driving_log.csv: \\n{}'.format(training_data.head(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data sets split and converted.\n"
     ]
    }
   ],
   "source": [
    "X_train = training_data[['left','center','right']]\n",
    "Y_train = training_data['steering']\n",
    "\n",
    "# Split training, validation, and test sets\n",
    "X_train, X_valid, Y_train, Y_valid = train_test_split(X_train, Y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "# Remove pandas index and convert to matrix\n",
    "X_left  = X_train['left'].as_matrix()\n",
    "X_right = X_train['right'].as_matrix()\n",
    "X_train = X_train['center'].as_matrix()\n",
    "X_valid = X_valid['center'].as_matrix()\n",
    "\n",
    "# Remove pandas index and convert to float32 matrix\n",
    "Y_train = Y_train.as_matrix()\n",
    "Y_train = Y_train.astype(np.float32)\n",
    "Y_valid = Y_valid.as_matrix()\n",
    "Y_valid = Y_valid.astype(np.float32)\n",
    "\n",
    "print('Data sets split and converted.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data sets shuffled.\n"
     ]
    }
   ],
   "source": [
    "# Shuffle data\n",
    "X_left, X_right, X_train, Y_train = shuffle(X_left, X_right, X_train, Y_train, random_state=0)\n",
    "print('Data sets shuffled.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_next_image(ex, value, X_train, X_left, X_right, Y_train):\n",
    "    offset = 1.2\n",
    "    dist = 20.0\n",
    "    steering = Y_train[ex]\n",
    "    \n",
    "    if value == 0:\n",
    "        image = plt.imread(folder + X_left[ex].replace(' ',''))\n",
    "        dsteering = offset / dist * 360 / (2 * np.pi) / 25.0\n",
    "        steering += dsteering\n",
    "    elif value == 1:\n",
    "        image = plt.imread(folder + X_train[ex].replace(' ',''))\n",
    "    elif value == 2:\n",
    "        image = plt.imread(folder + X_right[ex].replace(' ',''))\n",
    "        dsteering = -offset / dist * 360 / ( 2 * np.pi) / 25.0\n",
    "        steering += dsteering\n",
    "    else:\n",
    "        print ('Invalid value :', value )\n",
    "    \n",
    "    return image, steering\n",
    "\n",
    "def random_crop(image, steering=0.0, tx_lower=-20, tx_upper=20, ty_lower=-2, ty_upper=2, rand=True):\n",
    "    shape = image.shape\n",
    "    col_start, col_end = abs(tx_lower), shape[1]-tx_upper\n",
    "    horizon = 60;\n",
    "    bonnet = 136\n",
    "    if rand:\n",
    "        tx= np.random.randint(tx_lower,tx_upper+1)\n",
    "        ty= np.random.randint(ty_lower,ty_upper+1)\n",
    "    else:\n",
    "        tx, ty = 0, 0\n",
    "    \n",
    "    random_crop = image[horizon + ty : bonnet + ty, col_start + tx : col_end + tx, :]\n",
    "    image = cv2.resize(random_crop, (64,64), cv2.INTER_AREA)\n",
    "     \n",
    "    if tx_lower != tx_upper:\n",
    "        dsteering = -tx/(tx_upper-tx_lower)/3.0\n",
    "    else:\n",
    "        dsteering = 0\n",
    "    steering += dsteering\n",
    "    \n",
    "    return image, steering\n",
    "\n",
    "def random_shear(image,steering,shear_range):\n",
    "    rows, cols, ch = image.shape\n",
    "    dx = np.random.randint(-shear_range, shear_range + 1)\n",
    "    random_point = [cols / 2 + dx, rows / 2]\n",
    "    p1 = np.float32([[0, rows],[cols, rows],[cols / 2, rows / 2]])\n",
    "    p2 = np.float32([[0, rows],[cols, rows],random_point])\n",
    "    dsteering = dx / (rows / 2) * 360 / (2 * np.pi * 25.0) / 6.0    \n",
    "    M = cv2.getAffineTransform(p1, p2)\n",
    "    image = cv2.warpAffine(image, M, (cols, rows), borderMode=1)\n",
    "    steering += dsteering\n",
    "    return image, steering\n",
    "\n",
    "def random_flip(image, steering):\n",
    "    coin = np.random.randint(0, 2)\n",
    "    if coin==0:\n",
    "        image, steering = cv2.flip(image, 1), -steering\n",
    "    return image, steering\n",
    "        \n",
    "def generate_training_example(X_train, X_left, X_right, Y_train):\n",
    "    ex = np.random.randint(0, len(Y_train))\n",
    "    value = np.random.randint(0, 3)\n",
    "    image, steering = read_next_image(ex, value, X_train, X_left, X_right, Y_train)\n",
    "    image, steering = random_shear(image, steering, shear_range=100)   \n",
    "    image, steering = random_crop(image, steering, tx_lower=-20, tx_upper=20, ty_lower=-10, ty_upper=10)\n",
    "    image, steering = random_flip(image, steering)\n",
    "    return image,steering\n",
    "\n",
    "def get_validation_set(X_valid, Y_valid):\n",
    "    X = np.zeros((len(X_valid), 66, 200, 3))\n",
    "    Y = np.zeros(len(X_valid))\n",
    "    for i in range(len(X_valid)):\n",
    "        x, y = read_next_image(i, 1, X_valid, X_valid, X_valid, Y_valid)\n",
    "        X[i], Y[i] = random_crop(x, y, tx_lower=0, tx_upper=0, ty_lower=0, ty_upper=0)\n",
    "    return X, Y\n",
    "    \n",
    "def generate_train_batch(X_train, X_left, X_right, Y_train, batch_size=32):\n",
    "    batch_images = np.zeros((batch_size, 66, 200, 3))\n",
    "    batch_steering = np.zeros(batch_size)\n",
    "    while 1:\n",
    "        for i_batch in range(batch_size):\n",
    "            x, y = generate_training_example(X_train, X_left, X_right, Y_train)\n",
    "            batch_images[i_batch] = x\n",
    "            batch_steering[i_batch] = y\n",
    "        yield batch_images, batch_steering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train data type : object\n",
      "Y_train data type : float32\n",
      "X_valid data type : float64\n",
      "Y_valid data type : float64\n"
     ]
    }
   ],
   "source": [
    "batch_size = 200\n",
    "train_generator = generate_train_batch(X_train, X_left, X_right, Y_train, batch_size)\n",
    "X_valid, Y_valid = get_validation_set(X_valid, Y_valid)\n",
    "\n",
    "print('X_train data type :', X_train.dtype)\n",
    "print('Y_train data type :', Y_train.dtype)\n",
    "print('X_valid data type :', X_valid.dtype)\n",
    "print('Y_valid data type :', Y_valid.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "lambda_3 (Lambda)                (None, 64, 64, 3)     0           lambda_input_3[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_11 (Convolution2D) (None, 13, 13, 3)     118803      lambda_3[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "activation_11 (Activation)       (None, 13, 13, 3)     0           convolution2d_11[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_12 (Convolution2D) (None, 3, 3, 24)      218760      activation_11[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_12 (Activation)       (None, 3, 3, 24)      0           convolution2d_12[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_13 (Convolution2D) (None, 1, 1, 36)      568548      activation_12[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_13 (Activation)       (None, 1, 1, 36)      0           convolution2d_13[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_14 (Convolution2D) (None, 1, 1, 48)      190128      activation_13[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_14 (Activation)       (None, 1, 1, 48)      0           convolution2d_14[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_15 (Convolution2D) (None, 1, 1, 64)      55360       activation_14[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_15 (Activation)       (None, 1, 1, 64)      0           convolution2d_15[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)              (None, 64)            0           activation_15[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_9 (Dense)                  (None, 1164)          75660       flatten_3[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_10 (Dense)                 (None, 100)           116500      dense_9[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_11 (Dense)                 (None, 50)            5050        dense_10[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_12 (Dense)                 (None, 1)             51          dense_11[0][0]                   \n",
      "====================================================================================================\n",
      "Total params: 1,348,860\n",
      "Trainable params: 1,348,860\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Lambda(lambda x: x / 255 - 1.0, input_shape=(64,64,3)))\n",
    "model.add(Convolution2D(3, 66, 200, border_mode='same', subsample=(5,5)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(24, 31, 98, border_mode='same', subsample=(5,5)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(36, 14, 47, border_mode='same', subsample=(5,5)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(48, 5, 22, border_mode='same', subsample=(3,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(64, 1, 18, border_mode='same', subsample=(3,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1164))\n",
    "model.add(Dense(100))\n",
    "model.add(Dense(50))\n",
    "model.add(Dense(1))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model.json & model.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/35\n",
      "20000/20000 [==============================] - 56s - loss: 0.1036 - val_loss: 0.0232\n",
      "Epoch 2/35\n",
      "20000/20000 [==============================] - 46s - loss: 0.0540 - val_loss: 0.0321\n",
      "Epoch 3/35\n",
      "20000/20000 [==============================] - 43s - loss: 0.0411 - val_loss: 0.0285\n",
      "Epoch 4/35\n",
      "20000/20000 [==============================] - 41s - loss: 0.0348 - val_loss: 0.0227\n",
      "Epoch 5/35\n",
      "20000/20000 [==============================] - 41s - loss: 0.0326 - val_loss: 0.0185\n",
      "Epoch 6/35\n",
      "20000/20000 [==============================] - 41s - loss: 0.0309 - val_loss: 0.0192\n",
      "Epoch 7/35\n",
      "20000/20000 [==============================] - 41s - loss: 0.0293 - val_loss: 0.0160\n",
      "Epoch 8/35\n",
      "20000/20000 [==============================] - 41s - loss: 0.0285 - val_loss: 0.0164\n",
      "Epoch 9/35\n",
      "20000/20000 [==============================] - 41s - loss: 0.0269 - val_loss: 0.0169\n",
      "Epoch 10/35\n",
      "20000/20000 [==============================] - 41s - loss: 0.0263 - val_loss: 0.0191\n",
      "Epoch 11/35\n",
      "20000/20000 [==============================] - 40s - loss: 0.0261 - val_loss: 0.0168\n",
      "Epoch 12/35\n",
      "20000/20000 [==============================] - 41s - loss: 0.0248 - val_loss: 0.0215\n",
      "Epoch 13/35\n",
      "20000/20000 [==============================] - 40s - loss: 0.0253 - val_loss: 0.0157\n",
      "Epoch 14/35\n",
      "20000/20000 [==============================] - 40s - loss: 0.0245 - val_loss: 0.0164\n",
      "Epoch 15/35\n",
      "20000/20000 [==============================] - 40s - loss: 0.0228 - val_loss: 0.0172\n",
      "Epoch 16/35\n",
      "20000/20000 [==============================] - 40s - loss: 0.0235 - val_loss: 0.0175\n",
      "Epoch 17/35\n",
      "20000/20000 [==============================] - 40s - loss: 0.0234 - val_loss: 0.0153\n",
      "Epoch 18/35\n",
      "20000/20000 [==============================] - 40s - loss: 0.0232 - val_loss: 0.0154\n",
      "Epoch 19/35\n",
      "20000/20000 [==============================] - 40s - loss: 0.0230 - val_loss: 0.0151\n",
      "Epoch 20/35\n",
      "20000/20000 [==============================] - 41s - loss: 0.0227 - val_loss: 0.0152\n",
      "Epoch 21/35\n",
      "20000/20000 [==============================] - 40s - loss: 0.0228 - val_loss: 0.0155\n",
      "Epoch 22/35\n",
      "20000/20000 [==============================] - 41s - loss: 0.0228 - val_loss: 0.0149\n",
      "Epoch 23/35\n",
      "20000/20000 [==============================] - 40s - loss: 0.0227 - val_loss: 0.0157\n",
      "Epoch 24/35\n",
      "20000/20000 [==============================] - 41s - loss: 0.0211 - val_loss: 0.0144\n",
      "Epoch 25/35\n",
      "20000/20000 [==============================] - 41s - loss: 0.0209 - val_loss: 0.0149\n",
      "Epoch 26/35\n",
      "20000/20000 [==============================] - 41s - loss: 0.0218 - val_loss: 0.0155\n",
      "Epoch 27/35\n",
      "20000/20000 [==============================] - 41s - loss: 0.0213 - val_loss: 0.0150\n",
      "Epoch 28/35\n",
      "20000/20000 [==============================] - 41s - loss: 0.0215 - val_loss: 0.0147\n",
      "Epoch 29/35\n",
      "20000/20000 [==============================] - 41s - loss: 0.0206 - val_loss: 0.0153\n",
      "Epoch 30/35\n",
      "20000/20000 [==============================] - 43s - loss: 0.0208 - val_loss: 0.0136\n",
      "Epoch 31/35\n",
      "20000/20000 [==============================] - 44s - loss: 0.0202 - val_loss: 0.0138\n",
      "Epoch 32/35\n",
      "20000/20000 [==============================] - 43s - loss: 0.0207 - val_loss: 0.0142\n",
      "Epoch 33/35\n",
      "20000/20000 [==============================] - 44s - loss: 0.0204 - val_loss: 0.0158\n",
      "Epoch 34/35\n",
      "20000/20000 [==============================] - 43s - loss: 0.0205 - val_loss: 0.0136\n",
      "Epoch 35/35\n",
      "20000/20000 [==============================] - 44s - loss: 0.0199 - val_loss: 0.0149\n",
      "Training complete. Saving model and weights...\n",
      "Model and weights saved to home directory.\n"
     ]
    }
   ],
   "source": [
    "model_json = 'model.json'\n",
    "model_weights = 'model.h5'\n",
    "adam = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "restart = True\n",
    "if os.path.isfile(model_json) and restart:\n",
    "    try:\n",
    "        with open(model_json) as jfile:\n",
    "            model = model_from_json(json.load(jfile))\n",
    "            model.load_weights(model_weights)    \n",
    "        print('Loading trained model ...')\n",
    "    except Exception as e:\n",
    "        print('Unable to load model.', model_name, ':', e)\n",
    "        raise\n",
    "\n",
    "model.compile(optimizer=adam, loss='mse')\n",
    "\n",
    "EPOCHS = 35\n",
    "history = model.fit_generator(train_generator,\n",
    "                              samples_per_epoch=20000,\n",
    "                              nb_epoch=EPOCHS,\n",
    "                              validation_data=(X_valid,Y_valid),\n",
    "                              verbose=1)\n",
    "\n",
    "json_string = model.to_json()\n",
    "\n",
    "print('Training complete. Saving model and weights...')\n",
    "\n",
    "try:\n",
    "    os.remove(model_json)\n",
    "    os.remove(model_weights)\n",
    "except OSError:\n",
    "    pass   \n",
    "\n",
    "with open(model_json, 'w') as outfile:\n",
    "    json.dump(json_string, outfile)\n",
    "model.save_weights(model_weights)\n",
    "\n",
    "print('Model and weights saved to home directory.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:carnd-term1]",
   "language": "python",
   "name": "conda-env-carnd-term1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
